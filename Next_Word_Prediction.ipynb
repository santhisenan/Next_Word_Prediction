{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras as k\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Systems and methods are disclosed to recognize...</td>\n",
       "      <td>3D convolutional neural networks for automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A supervised procedure for obtaining weight va...</td>\n",
       "      <td>Accelerated training apparatus for back propag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method of accelerating the training of an ar...</td>\n",
       "      <td>Accelerating learning in neural networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An apparatus is described herein. The apparatu...</td>\n",
       "      <td>Accumulator constrained quantization of convol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract  \\\n",
       "0  \" This invention is a novel high-speed neural ...   \n",
       "1  Systems and methods are disclosed to recognize...   \n",
       "2  A supervised procedure for obtaining weight va...   \n",
       "3  A method of accelerating the training of an ar...   \n",
       "4  An apparatus is described herein. The apparatu...   \n",
       "\n",
       "                                        patent_title  \n",
       "0  \"Electronic neural network for solving \"\"trave...  \n",
       "1  3D convolutional neural networks for automatic...  \n",
       "2  Accelerated training apparatus for back propag...  \n",
       "3           Accelerating learning in neural networks  \n",
       "4  Accumulator constrained quantization of convol...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dict, word_idx, idx_word, sequences = get_data('./dataset/dataset.csv', training_len = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patent_abstract', 'patent_title'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = data['patent_abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Methods and apparatuses for backlash compensation. A dynamics inversion compensation scheme is designed for control of nonlinear discrete-time systems with input backlash. The techniques of this disclosure extend the dynamic inversion technique to discrete-time systems by using a filtered prediction'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[100][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = None, filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n', lower = False, split = ' ')\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "sequences = tokenizer.texts_to_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[885, 4, 3034, 7, 3035, 6565, 17, 1287, 2604, 2605, 977, 8, 658, 7, 51]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[100][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Methods and apparatuses for backlash compensation. A dynamics inversion compensation scheme is designed for control of nonlinear discrete time systems with input backlash. The techniques of this disclosure extend the dynamic inversion technique to discrete time systems by using a filtered prediction,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping indexes to words using index words\n",
    "idx_word = tokenizer.index_word\n",
    "\n",
    "' '.join(idx_word[w] for w in sequences[100][:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(idx_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "for seq in sequences:\n",
    "#     create multiple training examples form each sequence\n",
    "    for i in range(training_length, len(seq)):\n",
    "        extract = seq[i - training_length:i + 1]\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "        \n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84154, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84154, 10132)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding the labels\n",
    "num_words = len(idx_word) + 1\n",
    "label_array = np.zeros((len(features), num_words), dtype = np.int8)\n",
    "\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_array[example_index, word_index] = 1\n",
    "    \n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10132\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = './dataset/glove.6B/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "for i, word in enumerate(idx_word.keys()):\n",
    "     vector = word_lookup.get(idx_word[word], None)\n",
    "\n",
    "     if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(idx_word.keys()):\n",
    "    print(word_lookup.get(idx_word[word], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8047e-02,  3.1755e-01, -1.3962e-01,  8.9352e-01,  9.1169e-01,\n",
       "       -1.2790e-01, -3.6617e-01,  5.7105e-01, -2.7472e-01,  1.1765e-02,\n",
       "        1.7179e-01,  2.0147e-01,  8.3605e-01, -1.4204e-01,  2.9901e-01,\n",
       "        6.5877e-01, -1.3978e-01,  2.9055e-01, -5.5154e-02, -6.4442e-01,\n",
       "        6.9977e-01,  3.7503e-01,  7.7342e-01, -7.8217e-02,  2.0665e-01,\n",
       "       -1.1075e-01,  1.6104e-02, -5.5733e-02,  3.8644e-01,  1.1022e-01,\n",
       "       -9.5521e-02,  9.0990e-02, -4.2277e-01, -8.0655e-01,  8.5612e-01,\n",
       "        2.6637e-01,  3.3804e-01, -4.8534e-01,  7.0720e-02,  6.5147e-01,\n",
       "        9.2254e-02, -2.5710e-01, -3.5629e-01,  3.1867e-01, -3.6363e-01,\n",
       "        8.7018e-02,  5.7926e-01, -2.4927e-01,  1.4656e-03,  5.3542e-01,\n",
       "        9.2430e-02,  5.2260e-01,  2.6866e-01,  7.6084e-01, -9.3371e-02,\n",
       "       -1.7127e+00,  5.8050e-01,  4.7028e-01,  1.2986e+00,  4.1722e-01,\n",
       "       -6.1304e-01,  4.3239e-01,  7.3693e-01,  1.7390e-01,  6.7784e-01,\n",
       "       -3.1818e-02, -4.3835e-01,  2.0988e-01,  1.4622e-01,  1.2311e-01,\n",
       "       -2.0344e-01,  8.0864e-02,  7.3017e-01, -4.3548e-01,  4.8279e-01,\n",
       "       -2.8742e-01,  2.8475e-01, -1.8466e-01,  9.0431e-02, -2.1326e-01,\n",
       "       -1.6863e-01,  7.0717e-01, -4.4972e-01,  2.5431e-01, -1.2120e+00,\n",
       "        8.1380e-01, -1.2246e-01, -4.9838e-01,  9.5978e-01, -5.3628e-01,\n",
       "       -3.2954e-01,  5.3409e-01, -8.4908e-01,  2.1876e-01,  2.7415e-01,\n",
       "       -4.6630e-01,  2.3385e-01, -4.1242e-01,  6.3711e-01,  1.6328e-01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lookup.get('parallel', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parallel'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_word[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8047e-02,  3.1755e-01, -1.3962e-01,  8.9352e-01,  9.1169e-01,\n",
       "       -1.2790e-01, -3.6617e-01,  5.7105e-01, -2.7472e-01,  1.1765e-02,\n",
       "        1.7179e-01,  2.0147e-01,  8.3605e-01, -1.4204e-01,  2.9901e-01,\n",
       "        6.5877e-01, -1.3978e-01,  2.9055e-01, -5.5154e-02, -6.4442e-01,\n",
       "        6.9977e-01,  3.7503e-01,  7.7342e-01, -7.8217e-02,  2.0665e-01,\n",
       "       -1.1075e-01,  1.6104e-02, -5.5733e-02,  3.8644e-01,  1.1022e-01,\n",
       "       -9.5521e-02,  9.0990e-02, -4.2277e-01, -8.0655e-01,  8.5612e-01,\n",
       "        2.6637e-01,  3.3804e-01, -4.8534e-01,  7.0720e-02,  6.5147e-01,\n",
       "        9.2254e-02, -2.5710e-01, -3.5629e-01,  3.1867e-01, -3.6363e-01,\n",
       "        8.7018e-02,  5.7926e-01, -2.4927e-01,  1.4656e-03,  5.3542e-01,\n",
       "        9.2430e-02,  5.2260e-01,  2.6866e-01,  7.6084e-01, -9.3371e-02,\n",
       "       -1.7127e+00,  5.8050e-01,  4.7028e-01,  1.2986e+00,  4.1722e-01,\n",
       "       -6.1304e-01,  4.3239e-01,  7.3693e-01,  1.7390e-01,  6.7784e-01,\n",
       "       -3.1818e-02, -4.3835e-01,  2.0988e-01,  1.4622e-01,  1.2311e-01,\n",
       "       -2.0344e-01,  8.0864e-02,  7.3017e-01, -4.3548e-01,  4.8279e-01,\n",
       "       -2.8742e-01,  2.8475e-01, -1.8466e-01,  9.0431e-02, -2.1326e-01,\n",
       "       -1.6863e-01,  7.0717e-01, -4.4972e-01,  2.5431e-01, -1.2120e+00,\n",
       "        8.1380e-01, -1.2246e-01, -4.9838e-01,  9.5978e-01, -5.3628e-01,\n",
       "       -3.2954e-01,  5.3409e-01, -8.4908e-01,  2.1876e-01,  2.7415e-01,\n",
       "       -4.6630e-01,  2.3385e-01, -4.1242e-01,  6.3711e-01,  1.6328e-01])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10132, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              input_shape =(50,),\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "#                dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 50, 100)           1013200   \n",
      "_________________________________________________________________\n",
      "masking_11 (Masking)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10132)             658580    \n",
      "=================================================================\n",
      "Total params: 1,718,180\n",
      "Trainable params: 704,980\n",
      "Non-trainable params: 1,013,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "84154/84154 [==============================] - 105s 1ms/step - loss: 8.5384 - acc: 0.0643\n",
      "Epoch 2/2\n",
      "84154/84154 [==============================] - 103s 1ms/step - loss: 7.3602 - acc: 0.0216\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(features,  label_array, \n",
    "                    batch_size=2048, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.018e+03 2.877e+03 6.943e+03 ... 7.030e+02 2.000e+00 2.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "test_input = []\n",
    "test = np.zeros((50, 50))\n",
    "\n",
    "import random\n",
    "for x in range(50):\n",
    "  test_input.append(random.randint(1,10132))\n",
    "for i in range(48):\n",
    "    test[0][i] = test_input[i+2]\n",
    "test[0][48]=2\n",
    "test[0][49]=2\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[3.3466634e-04 9.9915624e-01 9.9913943e-01 ... 6.1692698e-03 3.2237084e-03\n",
      " 2.1323897e-03]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(word))\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "print(idx_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
